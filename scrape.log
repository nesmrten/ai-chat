import asyncio
import logging
import torch
import random
import requests
from urllib.parse import urlparse
from urllib import robotparser
from neo4j import GraphDatabase, basic_auth
from fake_useragent import UserAgent
from aiohttp import ClientSession, ClientError, ServerTimeoutError, TooManyRedirects
from tenacity import retry, stop_after_attempt, wait_fixed
from parsel import Selector
import spacy
from transformers import pipeline
from uuid import uuid4
from dotenv import load_dotenv
import os

# Load environment variables from .env file
load_dotenv()

NEO4J_URI = os.getenv("NEO4J_URI")
NEO4J_USER = os.getenv("NEO4J_USER")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD")
LOG_FILE = os.getenv("LOG_FILE")
REQUEST_DELAY = int(os.getenv("REQUEST_DELAY"))
CRAWL_DELAY = int(os.getenv("CRAWL_DELAY"))
MAX_RETRIES = int(os.getenv("MAX_RETRIES"))
SEMAPHORE_LIMIT = int(os.getenv("SEMAPHORE_LIMIT"))
USER_AGENTS = [ua.strip() for ua in os.getenv("USER_AGENTS").split(",")]

logging.basicConfig(
    filename=LOG_FILE,
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

chatbot = pipeline("text-generation", model="microsoft/DialoGPT-large", device=0 if torch.cuda.is_available() else -1)


def search_the_web(question):
    url = "https://api.duckduckgo.com"
    params = {
        "q": question,
        "format": "json",
        "pretty": 1,
    }

    response = requests.get(url, params=params)
    response.raise_for_status()

    return response.json().get('Abstract', '')


class Neo4jDatabase:
    def __init__(self, uri, user, password):
        self.driver = GraphDatabase.driver(uri, auth=basic_auth(user, password))

    def close(self):
        self.driver.close()

    def run_query(self, query, **kwargs):
        with self.driver.session() as session:
            try:
                session.run(query, **kwargs)
            except Exception as e:
                logging.error(f"Exception during database operation: {str(e)}")
                return None


class Scraper:
    def __init__(self, database, nlp):
        self.database = database
        self.nlp = nlp
        self.semaphore = asyncio.Semaphore(SEMAPHORE_LIMIT)

    async def fetch_url(self, session, url):
        headers = {'User-Agent': random.choice(USER_AGENTS)}
        try:
            async with self.semaphore, session.get(url, headers=headers) as response:
                response.raise_for_status()
                return await response.text()
        except Exception as e:
            logging.error(f"Exception during request: {str(e)}")
            return None

    async def can_fetch(self, url):
        robots_url = f"{urlparse(url).scheme}://{urlparse(url).netloc}/robots.txt"
        rp = robotparser.RobotFileParser()
        rp.set_url(robots_url)
        rp.read()
        return rp.can_fetch('*', url)

    async def parse_page(self, html, language):
        sel = Selector(html)
        text_content = ' '.join(sel.css('p::text, h1::text, h2::text, h3::text, h4::text, h5::text, h6::text').getall())
        nlp = self.nlp.get(language)
        doc = nlp(text_content)
        qna_pairs = []
        for sentence in doc.sents:
            if sentence.ents:
                for ent in sentence.ents:
                    qna_pairs.append({
                        'question': f"What is the {ent.label_} in the sentence '{sentence}'?",
                        'answer': ent.text
                    })
        return qna_pairs

    @retry(stop=stop_after_attempt(MAX_RETRIES), wait=wait_fixed(REQUEST_DELAY))
    async def scrape_website(self, url, language):
        async with ClientSession() as session:
            if not await self.can_fetch(url):
                logging.warning(f"Blocked by robots.txt: {url}")
                return []
            html = await self.fetch_url(session, url)
            if html:
                return await self.parse_page(html, language)
            return []

    async def insert_qna_pairs(self, qna_pairs, language):
        for pair in qna_pairs:
            question_text = pair['question']
            answer_text = pair['answer']

            # Check if the question already exists
            query = (
                "MATCH (q:Question {question_text: $question_text}) "
                "RETURN q.question_id AS question_id"
            )
            result = self.database.run_query(query, question_text=question_text)
            existing_question = None  # Added this line
            if result:
                existing_question = result.single()

            if existing_question is not None:
                question_id = existing_question["question_id"]
            else:
                question_id = str(uuid4())
                # Create a new question node
                query = (
                    "CREATE (q:Question {question_id: $question_id, question_text: $question_text})"
                )
                self.database.run_query(query, question_id=question_id, question_text=question_text)

            # Check if the answer already exists
            query = (
                "MATCH (a:Answer {answer_text: $answer_text}) "
                "RETURN a.answer_id AS answer_id"
            )
            result = self.database.run_query(query, answer_text=answer_text)
            existing_answer = None  # Added this line
            if result:
                existing_answer = result.single()

            if existing_answer is not None:
                answer_id = existing_answer["answer_id"]
            else:
                answer_id = str(uuid4())
                # Create a new answer node
                query = (
                    "CREATE (a:Answer {answer_id: $answer_id, answer_text: $answer_text})"
                )
                self.database.run_query(query, answer_id=answer_id, answer_text=answer_text)

            # Create a relationship between the question and answer
            query = (
                "MATCH (q:Question {question_id: $question_id}), (a:Answer {answer_id: $answer_id}) "
                "MERGE (q)-[:ANSWER]->(a)"
            )
            self.database.run_query(query, question_id=question_id, answer_id=answer_id)

    async def scrape_and_store(self, url, language):
        qna_pairs = await self.scrape_website(url, language)
        await self.insert_qna_pairs(qna_pairs, language)


async def main():
    logging.info('Starting Scraper')

    db = Neo4jDatabase(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)

    nlp_models = {
        'en': spacy.load('en_core_web_sm')
    }

    scraper = Scraper(db, nlp_models)

    urls_languages = [
        ('https://pc-pro.si', 'en'),
    ]

    tasks = [scraper.scrape_and_store(url, language) for url, language in urls_languages]

    await asyncio.gather(*tasks)

    logging.info('Finished Scraper')
    db.close()
    logging.info('Database connection closed')

    # Add some sample question to test the function
    question = "What is HTML?"
    answer = search_the_web(question)
    print(f"Question: {question}")
    print(f"Answer: {answer}")


if __name__ == "__main__":
    asyncio.run(main())
2023-05-24 19:32:56,185 - INFO - Starting Scraper
2023-05-24 19:32:58,488 - INFO - Finished Scraper
2023-05-24 19:32:58,489 - INFO - Database connection closed
2023-05-24 19:44:00,272 - INFO - Starting Scraper
2023-05-24 19:44:01,200 - INFO - Finished Scraper
2023-05-24 19:44:01,201 - INFO - Database connection closed
2023-05-24 19:51:18,316 - INFO - Starting Scraper
2023-05-24 19:51:19,227 - INFO - Finished Scraper
2023-05-24 19:51:19,228 - INFO - Database connection closed
2023-05-24 19:55:19,496 - INFO - Starting Scraper
2023-05-24 19:55:20,420 - INFO - Finished Scraper
2023-05-24 19:55:20,421 - INFO - Database connection closed
2023-05-24 19:59:57,819 - INFO - Starting Scraper
2023-05-24 19:59:58,699 - INFO - Finished Scraper
2023-05-24 19:59:58,699 - INFO - Database connection closed
2023-05-24 20:06:15,410 - INFO - Starting Scraper
2023-05-24 20:06:16,328 - INFO - Finished Scraper
2023-05-24 20:06:16,329 - INFO - Database connection closed
2023-05-24 20:07:56,784 - INFO - Starting Scraper
2023-05-24 20:07:57,676 - INFO - Finished Scraper
2023-05-24 20:07:57,677 - INFO - Database connection closed
2023-05-24 20:10:39,188 - INFO - Starting Scraper
2023-05-24 20:10:40,084 - INFO - Finished Scraper
2023-05-24 20:10:40,085 - INFO - Database connection closed
2023-05-24 21:47:54,916 - INFO - Starting Scraper
2023-05-24 21:47:55,840 - INFO - Finished Scraper
2023-05-24 21:47:55,840 - INFO - Database connection closed
2023-05-24 23:01:44,954 - INFO - Starting Scraper
2023-05-24 23:01:45,872 - INFO - Finished Scraper
2023-05-24 23:01:45,873 - INFO - Database connection closed
